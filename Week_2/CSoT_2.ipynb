{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqNNadlU7Moi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation functions and derivatives\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_deriv(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "def relu_deriv(x):\n",
        "    return (x > 0).astype(float)\n",
        "# Loss function: Binary Cross-Entropy\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    epsilon = 1e-8  # prevent log(0)\n",
        "    return -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
        "\n",
        "def binary_cross_entropy_deriv(y_true, y_pred):\n",
        "    epsilon = 1e-8\n",
        "    return (-(y_true / (y_pred + epsilon)) + ((1 - y_true) / (1 - y_pred + epsilon))) / len(y_true)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, activation='sigmoid', lr=0.1):\n",
        "        self.lr = lr\n",
        "        # Weight initialization\n",
        "        self.W1 = np.random.randn(input_size, hidden_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "        # Set activation\n",
        "        if activation == 'relu':\n",
        "            self.act = relu\n",
        "            self.act_deriv = relu_deriv\n",
        "        elif activation == 'sigmoid':\n",
        "            self.act = sigmoid\n",
        "            self.act_deriv = sigmoid_deriv\n",
        "        else:\n",
        "            raise ValueError(\"Only 'relu' and 'sigmoid' supported.\")\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.Z1 = X @ self.W1 + self.b1\n",
        "        self.A1 = self.act(self.Z1)\n",
        "        self.Z2 = self.A1 @ self.W2 + self.b2\n",
        "        self.A2 = sigmoid(self.Z2)  # output layer uses sigmoid for binary classification\n",
        "        return self.A2\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        # Derivative of loss w.r.t output\n",
        "        dA2 = binary_cross_entropy_deriv(y, output)\n",
        "        dZ2 = dA2 * sigmoid_deriv(self.Z2)\n",
        "        dW2 = self.A1.T @ dZ2\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "        dA1 = dZ2 @ self.W2.T\n",
        "        dZ1 = dA1 * self.act_deriv(self.Z1)\n",
        "        dW1 = X.T @ dZ1\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights\n",
        "        self.W1 -= self.lr * dW1\n",
        "        self.b1 -= self.lr * db1\n",
        "        self.W2 -= self.lr * dW2\n",
        "        self.b2 -= self.lr * db2\n",
        "\n",
        "    def train(self, X, y, epochs=1000):\n",
        "        for i in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            loss = binary_cross_entropy(y, output)\n",
        "            self.backward(X, y, output)\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Epoch {i}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "vABPqY5n8B5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR input and output\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Try both ReLU and Sigmoid hidden layers\n",
        "print(\"Training with ReLU activation:\")\n",
        "nn_relu = NeuralNetwork(input_size=2, hidden_size=4, output_size=1, activation='relu', lr=0.1)\n",
        "nn_relu.train(X, y, epochs=1000)\n",
        "\n",
        "print(\"\\nTraining with Sigmoid activation:\")\n",
        "nn_sigmoid = NeuralNetwork(input_size=2, hidden_size=4, output_size=1, activation='sigmoid', lr=0.1)\n",
        "nn_sigmoid.train(X, y, epochs=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRL5dyoM2ZUz",
        "outputId": "79ea1abe-1781-4bbd-c770-fb51cd1c7306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with ReLU activation:\n",
            "Epoch 0, Loss: 0.6621\n",
            "Epoch 100, Loss: 0.5389\n",
            "Epoch 200, Loss: 0.5039\n",
            "Epoch 300, Loss: 0.4912\n",
            "Epoch 400, Loss: 0.4853\n",
            "Epoch 500, Loss: 0.4830\n",
            "Epoch 600, Loss: 0.4813\n",
            "Epoch 700, Loss: 0.4808\n",
            "Epoch 800, Loss: 0.4800\n",
            "Epoch 900, Loss: 0.4797\n",
            "\n",
            "Training with Sigmoid activation:\n",
            "Epoch 0, Loss: 1.0345\n",
            "Epoch 100, Loss: 0.7082\n",
            "Epoch 200, Loss: 0.7011\n",
            "Epoch 300, Loss: 0.6963\n",
            "Epoch 400, Loss: 0.6925\n",
            "Epoch 500, Loss: 0.6889\n",
            "Epoch 600, Loss: 0.6850\n",
            "Epoch 700, Loss: 0.6804\n",
            "Epoch 800, Loss: 0.6745\n",
            "Epoch 900, Loss: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch Implementation"
      ],
      "metadata": {
        "id": "Ktt7TASg4urO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# NumPy array\n",
        "a_np = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
        "\n",
        "# PyTorch tensor from NumPy\n",
        "a_torch = torch.tensor(a_np, requires_grad=True)\n",
        "\n",
        "print(a_torch)\n",
        "print(f\"Shape: {a_torch.shape}, Dtype: {a_torch.dtype}\")\n",
        "\n",
        "# Broadcasting works like NumPy\n",
        "b = torch.tensor([1.0, 2.0])\n",
        "print(a_torch + b)  # Broadcasting row-wise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm3mVbZY200H",
        "outputId": "ed908924-62bc-4c57-9e02-786051353e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]], dtype=torch.float64, requires_grad=True)\n",
            "Shape: torch.Size([2, 2]), Dtype: torch.float64\n",
            "tensor([[2., 4.],\n",
            "        [4., 6.]], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "y = x**2 + 3*x + 1  # simple function\n",
        "y.backward()        # computes dy/dx\n",
        "\n",
        "print(f\"dy/dx: {x.grad}\")  # Should be 2x + 3 = 7.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z8E7M7Z4y6m",
        "outputId": "9bbd5f6c-ff6c-4663-b81f-139b3321640b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dy/dx: tensor([7.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# XOR Data\n",
        "X = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "y = torch.tensor([[0.], [1.], [1.], [0.]])\n"
      ],
      "metadata": {
        "id": "l2fMgf3f40mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XORNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XORNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2, 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "k7fl8FzP419r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XORNet()\n",
        "loss_fn = nn.BCELoss()  # Binary cross-entropy\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Training\n",
        "for epoch in range(1000):\n",
        "    y_pred = model(X)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "X3qUEt4543gD",
        "outputId": "b60d80ae-c4a3-4774-827c-4e1b2bd2a048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.7091\n",
            "Epoch 100, Loss: 0.6892\n",
            "Epoch 200, Loss: 0.6690\n",
            "Epoch 300, Loss: 0.6158\n",
            "Epoch 400, Loss: 0.5316\n",
            "Epoch 500, Loss: 0.4412\n",
            "Epoch 600, Loss: 0.3223\n",
            "Epoch 700, Loss: 0.2220\n",
            "Epoch 800, Loss: 0.1544\n",
            "Epoch 900, Loss: 0.1107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    preds = model(X).round()\n",
        "    print(\"Predictions:\\n\", preds)"
      ],
      "metadata": {
        "id": "77dmQF3r44y6",
        "outputId": "8feecebd-0815-48b2-f415-688ecd8d1f2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            " tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YssdICql46rs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}